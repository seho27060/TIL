[TOC]

# 빅데이터를 지탱하는 기술

---

## 목차

1. 빅데이터의 기초 지식

2. 빅데이터의 검색

3. 빅데이터의 분산 처리

4. 빅데이터의 축적

5. 빅데이터의 파이프라인

6. 빅데이터 분석 기반의 구축

---

## 1장. 빅데이터의 기초 지식

### 1-1. [배경] 빅데이터의 정착

- 기술(하드웨어, 소프트웨어)의 발달로 인해, 이전에는 처리가 곤란한(버려지는) 대용량의 데이터를 처리할 수 있게 됐다.

### 1-2. 빅데이터 시대의 데이터 분석 기반

- 실시간 처리와 배치 처리, 두가지 방식의 데이터 파이프라인

- 데이터 웨어하우스(data warehouse, WDH) : 수집 raw 데이터를 사용 가능하도록 전처리하여 저장함. 장기간 보관

- 데이터 마트(data mart) : DWH에서 분석에 사용할 데이터를 추출, 처리한, BI를 위한 데이터 세트. 단기간 보관

- 데이터 레이크(data lake, DL) : 발생하는 raw 데이터를 전처리하지 않고 원래의 상태로 저장하는 스토리지

- DWH or DL 중심 데이터 파이프라인 구조
  
  - 데이터 소스
    
    - 데이터 웨어하우스 - 수집된 raw 데이터를 전처리하여 적재
    
    - 데이터 레이크 - 수집 raw 데이터를 원 상태 그대로 적재
  
  - 데이터 마트 - DWH(or DL)의 raw데이터 중 필요에 의해 추출한 데이터 세트
  
  각 요소는 '데이터ETL'로 처리된다.

### 1-3. [속성 학습] 스크립트 언어에 의한 특별 분석과 데이터 프레임

- `pandas`를 활용한 스몰 데이터 분석
  
  - 빅데이터에 대한 분산 처리는 지원 X

- `dataframe` 타입을 활용하여 EDA(exploratory data analysis)가 가능하다.
  
  - `dataframe`만의 데이터 조회 뿐만 아니라 `SQL`을 활용할 수 도 있다.

### 1-4. BI 도구와 모니터링

- 데이터를 시각화하기 위해 BI도구를 많이 사용한다.

- BI도구는 사용가능하도록 "전처리"된 데이터에 대해서만 시각화가 가능하다.

- BI 도구 활용을 위해 전처리 작업을 수행할때, **자동화**가 필요한 부분과 **수작업**해야 하는 경계를 잘 확인하자

## 2장. 빅데이터의 탐색

### 2-1. 크로스 집계의 기본

- 레코드중심의 크로스 테이블과 로우,컬럼 중심의 트랜잭션 테이블
  
  - 보통 데이터는 트랜잭션 테이블로 저장된다.

- 수집 데이터를 `SQL`,`pandas`로 집계(aggregation)하여 데이터 마트를 생성후 BI도구를 통해 시각화한다.
  
  - 시스템 구성은 '데이터 마트의 크기'에 따라 결정된다.
  
  - 너무 작은 데이터 마트는 데이터 손실로 정확한 시각화가 힘들고,
  
  - 너무 큰 데이터 마트는 집계의 의미가 무의미하게 자원을 많이 사용한다.
  
  - 어쩔 수 없는 트레이드 오프 관계이므로 필요에 따른 고려가 필요하다.

### 2-2. 열 지향 스토리지에 의한 고속화

- 수십 GB정도의 스몰 데이터는 대기시간에 대한 걱정이 없다..
  
  - 데이터의 증가에 따라 RDB에서의 지연시간이 발생한다면 어떻게 해야할까.

- **MPP**(massive parallel processing, 대규모 병렬 처리) 아키텍쳐를활용하자.
  
  - Amazon Redshift, Google BigQuery 등이 있다.
  
  - 압축과 분산을 통해 대규모 데이터에 대한 부하를 여러 디스크에 분산하여 데이터 로드에 따른 지연을 줄인다.

- 데이터 베이스의 접근 방식
  
  | 데이터 베이스      | 접근 방식                                                                                                                                              |
  | ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------- |
  | 행 지향 데이터 베이스 | 데이터 접근에 index를 활용한다. 마지막 행에 데이터를 추가하면 되므로 쓰기에 특화. 전체 데이터를 로드해야함.                                                                                   |
  | 열 지향 데이터 베이스 | 행의 index는 의미를 갖는 값이 할당된다. 보통 데이터분석에서는 테이블의 전체 데이터가 아닌 부분 데이터가 필요하다. 열 지향 데이터베이스의 경우 필요 컬럼에 대한 데이터를 조회하므로 행 지향 데이터베이스 보다 선택적 조회가 빠르다.               |
  | MPP 데이터 베이스  | MPP에서는 하나의 쿼리를 다수의 작은 태스크로 분해하고 이를 가능한 한 병렬로 실행한다. 1억 레코드로 이루어진 테이블에서 태스크를 10만 레코드로 구분하여 1000개의 태스크로 나눈다. 1000개 태스크에 대한 결과를 집계하여 모든 결과을 총합하여 산출한다. |

### 2-3. 애드 혹 분석과 시각화 도구

- 애드 혹(ad-hoc)
  
  1. 특정한 문제나 일을 위해 만들어진 관습적인 해결책
  2. 일반화할 수 없는 해결책
  3. 어떤 다른 목적에 적응시킬 수 없는 해결책
  
  정형화된 방법론이 아니라, 특정 상황을 위해서만 적용 가능한 방안 이라고 할 수 있겠다.

- '노트북'을 활용한 애드혹 분석, '대시보드'를 활용한 지속적 모니터링

- 필요에 의한 데이터 마트 구축이 선행된다.

### 2-4. 데이터 마트의 기본 구조

- 팩트 테이블 : 트랜잭션과 같은 사실이 기록된 테이블

- 디멘전 테이블 : 트랜잭션에 활용되는 정보가 저장된 데이터

- 비정규화 테이블 :  데이터 분석을 위해서는 정형화된 데이터 테이블을 분해하여 비정규화하는 과정으로 필요에 의한 여러개의 비정규화 테이블로 재생성한다.
  
  - 규모가 작다면 RDB도 적합하지만 아니라면, MPP 데이터베이스와 같은 열 지향 데이터베이스를 사용하자.

## 3장. 빅데이터의 분산 처리

- Hadoop And Spark

### 3-1. 대규모 분산 처리의 프레임워크

#### Hadoop

- `Hadoop` 기본 구성 요소
  
  - 분산 파일 시스템 - HDFS
  
  - 리소스 관리자 - `YARN`
  
  - 분산 데이터 처리 -` MapReduce`
  
  각 요소는 다른 애플리케이션으로 대체 가능함.

- 분산 데이터 및 처리 엔진
  
  - `MapReduce`의 경우 대용량 데이터를 배치 처리가 목적임
  
  - `Hive`는 사용자의 `SQL`을 적합한 `MapReduce`로 변환하여 실행시킨다.
  
  - 데이터분석을 위한 애드 혹에서는 단순 쿼리가 많이 사용되는데, 이는 대용량 처리에 적합한 `MapReduce`(`Hive`)로는 **오버 헤드**가 발생한다.
  
  - Hive on Tez
    
    - `MapReduce`에 기반한 Hive on MR에서 `Tez`로 넘어옴
    
    - `Tez`의 경우 순차적으로 작업을 처리((1회의 MR 스테이지 종료)하는 MR과 다르게 작업의 처리를 기다리지 않고 다음 작업을 진행한다.
    
    - Hive on Spark 또한 개발이 진행중이다. 

- 대화형 쿼리 엔진
  
  - `MR`, `Tez` 등은 비구조화 데이터를 장기간의 배치 처리를 한정된 리소스로 안정되게 수행한다.
  
  - 데이터분석에 사용되는 작은 규모의 반복적인 쿼리를 위해 대화형 쿼리 엔진 `Impala` 와 `presto`가 등장한다.
    
    - 구조화 데이터에 대한 쿼리를 수행하기에 적합하다.

- `Hadoop`에서는 다수의 쿼리 엔진이 개발되어 있다.
  
  - 이를 '**SQL on Hadoop**'이라고 부른다.

#### Spark

- 하드웨어 자원이 부족한 시기, `MapReduce`의 경우 처리에 disk를 활용함

- 하드웨어의 메모리 자원이 자연스레 증가함에 따라 작업에서 메모리를 적극 활용할 수 있게 되었고, `Spark`라는 프로젝트가 발달하게 됨

- `Spark`는 `Hadoop`을 대체하는게 아닌 `MapReduce`를 대체한다.(위의 `Hadoop` 기본 구성 요소 참고)

- 다양한 스크립트 언어로의 대응

- 내재된 `Spark SQL`과 `Spark Streaming`로 대화형 쿼리와 실시간 처리 기능 사용 가능.

### 3-2. 쿼리 엔진

- `Hive`에 의한 구조화 데이터 생성

- `Presto`에 의한 대화식 쿼리

#### 데이터 마트 구축의 파이프라인

- 원본 데이터를 `Hive`로 대용량 배치 처리로 열 지향 스토리지 형식으로 구조화 데이터로 처리한다.

- 구조화된 데이터를 집계, 결합하여 비정규화 테이블로 데이터 마트로 내보낸다.

- `Hive`에서 만든 각 테이블의 정보는 "Hive 메타 스토어"라는 특별한 데이터베이스에 저장된다.
  
  - 메타스토어는 다른  `SQL on Hadoop`의 쿼리 엔진에서도 참고하여 사용한다.

#### Hive에 의한 구조화 데이터 작성

- `Hive`를 이용한 **데이터 구조화 프로세스**
  
  - raw데이터를 `Hive`상에 불러온다.
  
  - 데이터를 열 지향 스토리지 형식(ORC, parquet)로 변환하여 새로운 테이블로 저장한다.(데이터 구조화)
  
  - 새로운 열 지향 테이블로 데이터에 대한 집계를 처리한다.
  
  데이터 구조화 작업에 시간이 걸릴 수 있지만, 이후 단편적 쿼리(집계)에 대한 소요 시간이 단축된다.

- `Hive`로 비정규화 테이블 작성하기
  
  - 데이터 구조화를 마친 테이블에서 **데이터 마트를 구축**한다.
  
  - 대용량 배치 처리는 `Hive`, 대화형 쿼리 엔진은 `Presto`를 선택한다.
  
  - 비정규화 테이블을 생성하는건 많은 자원이 필요하므로 **효율적으로 쿼리**를 날리는게 중요하다.

- `Hive`의 쿼리 개선하기
  
  - 서브 쿼리 안에서 레코드 수 줄이기 - 초기 단계에서 팩트 테이블 작게하기
    
    비효율적 쿼리 예시
    
    ```sql
    SELECT ...
    FROM DataTable a
    JOIN UserTable b ON b.id = a.user_id
    WHERE b.created_at = '2017-01-01'
    ```
    
    효율적 쿼리 예시
    
    ```sql
    SELECT ...
    FROM (
        SELECT * DataTable
        WHERE time >= TIMESTAMP '2017-01-01 00:00:00'
    ) a
    JOIN UserTable b ON b.id = a.user_id
    WHERE b.created_at = '2017-01-01'
    ```
    
    서브 쿼리로 팩트 테이블을 작게 하여 작업을 수행하는게 확실히 효율적이다.
    
    파이프라인의 구성중 **로그확인을 통해 비효율적 쿼리를 확인하여 개선**하는 작업 역시 필요하다.
  
  - 데이터 편향 피하기
    
    데이터가 균등하게 분포됐다는 가정하에 데이터를 중앙 집합시켜 중복 체크를 하는 `DISTINCT`나 분산처리가 가능한 `GROUP BY`는 문제가 없다.
    
    만약 특정 데이터(30일중 13일에 트래픽 증폭)에 데이터가 편향되었을 경우, 아무리 분산 처리가 되어도 데이터가 편향된 부분은 병목 현상이 발생한다.
    
    비효율적 쿼리
    
    ```sql
    SELECT date, count(distinct user_id) users
    FROM acess_log GROUP BY date
    ```
    
    `dinstinc count` 쿼리는 `GROUP BY`가 분산 처리한 데이터에 중복 처리를 한다.(`dinstinc`는 분산처리 되지 않음)
    효율적 쿼리
    
    ```sql
    SELECT date, count(*) users
    FROM (
        SELECT DISTINCT date, user_id FROM access_log
        ) t
    GROUP BY date
    ```
    
    서브 쿼리를 사용하여 `distict`로 중복이 이미 제거된 테이블에 `GROUP BY`와 같은 분산 처리를 할 경우, 데이터 편향에 의한 병목 현상을 방지 할 수 있다.

#### 대화형 쿼리 엔진 Presto의 구조

- 작은 쿼리를 여러번 실행하는 대화형 데이터 처리에 적합한 대화형 쿼리 엔진 `Presto`를 알아보자.
  
  (`Google Big Query`, `Impala`, `Drill`등 여러 소프트웨어가 있으니 환경과 조건에 따라 선택 사용하자)

- 플러그인 가능한 스토리지 
  
  - 전용 스토리지를 갖지 않으므로 여러 데이터 소스에서 직접 데이터를 읽어 들일 수 있다.
  
  - 구조화 데이터(Structed Data)에서 집계의 목적에 적합하다.
  
  - 하나의 쿼리 안에서 분산 스토리지 상의 팩트 테이블과 `MySQL`의 마스터 테이블을 조인하는 등 여러 데이터 소스에 연결 가능하다.

- CPU 처리의 최적화 
  
  - 작은 규모의 쿼리를 멀티 스레드화 하여 단일 머신으로 병렬하여 최적화된 실행이 가능하다.

- 인 메모리 처리에 의한 고속화 
  
  - 모든 데이터 처리를 메모리상으로 실행한다.

- 분산 결합과 브로드캐스트 결합
  
  - 분산 결합(distributed join)으로 같은 키를 갖는 데이터는 동일 노드에 모인다.
  
  - 브로드캐스트 결합(broadcast join)으로 충분히 작은 테이블를 갖는 노드에 다른 노드의 상대적으로 큰 테이블을 분할하여 데이터를 처리한다.(작은 테이블의 데이터는 복사하여 분할된 큰 테이블에 할당한다)
  
  - 분산 결합과 브로드캐스트 결합은 서로 양립할 수 없다.

#### 데이터 분석의 프레임워크 선택하기

- 많은 옵션(프레임워크)중 어떤 상황에 어떤 옵션을 선택해야 할까?

- MPP 데이터 베이스 - 완성한 비정규화 테이블의 고속 집계

- Hive - 데이터양에 좌우되지 않는 쿼리 엔진

- Presto - 속도 중시 & 대화식으로 특화된 쿼리 엔진

- Spark - 분산 시스템을 사용한 프로그래밍 환경 

### 3-3. 데이터 마트의 구축

- 데이터 마트 구축에 필요한 각종 테이블과 비정규화 테이블을 만들기까지의 흐름

#### 팩트 테이블

- 팩트 테이블 작성에는 추가(append)와 치환(replace) 두가지 방법이 있다.
  
  - 추가 - 데이터를 기존 테이블에 추가(`INSERT INTO`)
  
  - 치환 - 과거 데이터를 포함하여 새로운 테이블로 치환(`CREATE TABLE`)

- 테이블 파티셔닝(table partitioning)
  
  - 테이블을 여러 물리적인 파티션으로 나눔으로 추가로 발생하는 결손, 중복 등의 문제를 해결한다.

- 데이터 마트의 치환
  
  - 테이블을 새로 생성함으로 결손, 중복등의 문제가 발생하지 않는다.
  - 다만 생성에 소요 시간이 커질 수 있으므로 조건과 환경을 고려해야 한다.

#### 집계 테이블

- 팩트 테이블을 어느 정도 모아 집계한 집계 테이블(summary table)

- 집계 테이블로 그 크기를 작게하기 위해 카디널리티(cardinality)를 작게한다.
  
  - cardinality - 각 컬럼이 취하는 값의 범위(특정 기간의 데이터, 성별, 지역별 등)

#### 스냅샷 테이블

- 마스터 데이터처럼 업데이트 가능성이 있는 테이블에 대해 **정기적으로** 테이블을 **통째로** 저장하는 방법

- 스냅샷 테이블을 팩트 테이블과 결합함으로 디멘전 테이블로도 사용한다.

#### 이력 테이블

- 정기적으로 모든 데이터를 스냅샷하는게 아닌, 변경 데이터를 증분으로만 스냅샷하거나 변경이 있을 때마다 그 내용을 기록하는 이력 테이블(history table)

- 테이블의 크기가 비교적 작지만, 어느 순간의 마스터 테이블을 완전히 복원할 순 없다.

#### 디멘전을 추가하여 비정규화 테이블 완성시키기

- 팩트 테이블과 디멘전 테이블을 결합하여 비정규화 테이블을 만든다.

- 카디널리티가 작은 디멘전을 만들어 결합하고, 시각화에 필요하지 않는 칼럼을 가급적 제거함으로 시각화가 쉽게 데이터양이 적은 비정규화 테이블을 생성한다.

- 데이터 집계의 기본형
  
  - 시간에 의한 검색이나 참고하는 컬럼 수를 줄여 팩트 테이블에서 필요 데이터를 추출한다.
  
  - 디멘전 테이블과 결합하여 카디널리티를 작게 하여 데이터 마트에 저장할 컬럼을 선택한다.
  
  - 그룹화하여 측정값을 집계한다.
  
  - 예시
    
    ```sql
    SELECT 
        date_trunc('day',a.time) time,
        date_diff('day','b.min_time) days,
        count(*) count
    FROM (
        # 1. 팩트 테이블로부터 필요 컬럼 추출
        SELECT time, session_id FROM access_log
        # 집계 기간 검색으로 카디너리티 축소
        WHERE time BETWEEN TIMESTAMP '2017-01-01' AND TIMESTAMP '2018-01-01'
    ) a
    # 2. 디멘전 테이블과 결합
    JOIN sessions b ON b.session_id = a.session_id
    # 그룹화
    GROUP BY 1, 2
    ```

## 4장. 빅데이터의 축적

### 4-1. 벌크 형과 스트리밍 형의 데이터 수집

#### 객체 스토리지와 데이터 수집

- 빅데이터는 대부분의 경우 확장성이 높은 분산 스토리지(distributed storage)에 저장된다.

- 기본이 되는 객체 스토리지(object storage)가 있으며 Haddop이라면 HDFS, AWS의 S3가 유명하다.

- 데이터 수집(data ingestion)
  
  - 너무 작은 크기의 파일을 저장하거나
  
  - 너무 큰 크기의 파일 저장은 객체 스토리지에 비효율적이므로,
  
  - 작은 크기의 파일을 합치거나, 큰 크기의 파일을 분할하여 적당한 크기의 데이터로 저장하는게 옳다.

#### 벌크 형의 데이터 전송

- 전통적으로 벌크 형 데이터를 데이터 웨어하우스에 저장한다.

- 이러한 데이터 수집을 위한 **ETL서버**(ETL server)를 필요로 한다.

- 일정 주기로 데이터를 수집하여 데이터 전송의 신뢰성을 갖는다.

- 이러한 벌크 형 데이터 전송은 **워크플로 관리도구**의 사용이 적절하다.

#### 스트리밍 형의 데이터 전송

- '웹 브라우저', '모바일 앱', '디바이스' 등에서 즉각적으로 생성되는 데이터의 경우 스트리밍 데이터의 전송을 고려해볼만 하다.

- 다수의 클라이언트에서 계속해서 작은 데이터를 전송하는 **메시지 배송(message delivery)** 을 사용한다.
  
  - 데이터 전송량에 비해 높은 오버헤드가 발생하기 때문에, 서버의 높은 성능을 요구한다.

- NoSQL 데이터베이스가 사용되며 **메시지 큐(message queue)**, **메시지 브로커(message broker)** 등의 중계 시스템으로 일정 간격으로 수집 데이터를 분산 스토리지에 저장한다.

- 웹 브라우저에서의 메시지 배송
  
  - 'Fluetnd', 'Logstash'와 같은 상주형 로그 수집 소프트웨어가 사용된다.
  
  - 또는 자바스크립트로 웹 브라우저에서 직접 메시지를 보내는 **'웹 이벤트 추적(web event tracking)'** 을 사용한다.

- 모바일 앱으로부터의 메시지 배송
  
  - MBaaS(Mobile Backend as a Service)라는 백엔드의 각종 서비스로 벡엔드 데이터 저장소에 저장한 데이터를 벌크형 도구로 수집하거나
  
  - 모바일 용 개발 키드(SDK)로 온/오프라인 상태의 앱에서 SDK 내부에 축적된 데이터를 온라인에 연결시 데이터를 수집한다.

- 디바이스로부터의 메시지 배송
  
  - **'MQTT(MQ Telmetry Transport)'** 라는 TCP/IP로 데이터를 전송하는 프로토콜을 사용한다.
  
  - 일반적으로 **'Pub/Sub 형 메시지 배송(Pub/Sub message delivery)** 의 구조를 갖는다.
  
  - 관리자에 의해 생성된 **'Topic'** 을 클라이언트가 구독(Subscribe)하면 연결되며 메시지를 송수신한다.
  
  - 메시지의 교환을 중계하는 **'MQTT 브로커(MQTT broker)'** 서버와 메시지를 수진하는 **'MQTT 구독자(MQTT subscriber)** 시스템으로 구성된다.

#### [성능 X 신뢰성] 메시지 배송의 트레이드 오프

- 클라이언트가 많아질 수록 스트리밍 형의 메시지 배송은 '성능'과 '신뢰성'이 반비례 관계에 놓이게 된다. 그 이유를 알아보자

#### 메시지 브로커

- 서비스의 규모가 커질수록 메시지 배송의 부하는 커진다.

- 클라이언트의 메시지 전송 증가 -> 서버에서의 수신 실패 -> 클라이언트의 재전송 -> 수신 실패...의 과정이 반복적으로 수행되어 서버 부하는 갈수록 커진다.

- 이러한 부하로 인한 메시지 배송 실패를 방지하기 위해 **메시지 브로커**를 사용한다.
  
  - 오픈소스로 **Apache Kafka**와 클라우드 서비스로 **Amazon Kinesis**가 있다.

- 푸쉬 형과 풀 형
  
  - 송신 측의 제어로 데이터를 송신하는 **Push**
  
  - 수신 측의 주도로 데이터를 수신하는 **Pull**
  
  - 메시지 브로커는 데이터 쓰기 속도를 조정하기 위한 완충 부분으로 푸쉬 형에서 풀 형으로 메시지 배송의 타이밍을 변환한다.
  
  - 메시지 브로커에 데이터를 push하는 **생산자(producer)** 와 pull하는 **소비자(consumer)** 로 구성된다.

- 메시지 라우팅
  
  - 1분마다 총 200MB의 200만번의 데이터 전송이 발생하면 스토리지 서버는 200만의 쓰기 작업을 수행해야 하며 이는 매우 부하가 큰 작업이다.
  
  - 메시지 브로커는 이를 해결한다. 메시지 브로커로 200만번의 데이터 푸쉬가 이뤄지고, 이것을 '소비자'에서 풀한다. (**스트림 처리, stream processing**, 짧은 간격으로 차례대로 데이터를 꺼내서 처리함)
  
  - 메시지 브로커에 써넣은 데이터는 복수의 다른 소비자에서 읽어 들일 수 있다. 이를 통해 메시지가 복사되어 데이터를 여러 경로로 분기하는데 이를 '**메시지 라우팅**(message routing)'라고 한다.

### 메시지 배송을 확실하기 실시하는 것은 어렵다.

- 모바일 회선과 같은 낮은 신뢰성의 네트워크나 분산 시스템의 특성상 메시지의 중복과 누락이 발생하며 이는 **신뢰성(reliablity)** 와 직결된다.
  
  - 대부분의 경우 아래 3가지 중 1개를 보장하도록 설계된다.

- **at most once** - 메시지는 한 번만 전송된다. 도중에 전송 실패로 메시지 결손 가능성이 있다.
  
  - 무슨 일이 일어나도 절대로 메시지를 **재전송(retransmission)** 하지 않는다. 보내는 클라이언트가 한번 보내면 수신 확인 없이 해당 전송은 끝난다.

- **exactly once** - 메시지는 손실되거나 중복 없이 한 번만 전달된다.
  
  - 네트워크상의 두 개 노드의 양 방향 통신을 보장하기 위해 **코디네이터(coordinator)** 가 필수적이다.
  
  - 코디네이터로 exactly once가 가능하나..코디네이터의 shut down으로 인한 일시적 부재나, 코디네이터 의존에 의한 성능상의 문제가 발생한다.

- **at least once** - 메시지는 확실히 전달된다. 같은 것이 여러 번 전달된 가능성이 있다.
  
  - 메시지가 재전송되어도 **중복 제거(deduplication)** 하는 구조가 있다면 중복이 없는 것 처럼 보일 수 있다.
  
  - TCP/IP에 의한 네트워크 통신에서 TCP로 메시지의 수신 확인을 위한 'ack' 플래그로 'at least once'을 실현할 수 있다.
    
    - 메시지마다 패킷 번호를 부여하여 중복되는 패킷 번호의 메시지는 제거하는 방법.
  
  - 메시지 배송에 사용되는 오픈소스 소프트웨어는 'at least once'를 보장한다.. 하지만 중복제거는 사용자의 몫이므로 이를 주의하자.

#### 중복 제거는 높은 비용의 오퍼레이션

- TCP의 메시지 시퀀스 번호로 메시지 중복을 확인할 수 있으나.. 분산 시스템에서는 이를 확인하기 위한 중앙 처리가 필요하다. 
  
  - 힘들게 분산시켜놨는데 또 중앙처리하면.. 성능 하락으로 이어진다.
  
  - 이를 위한 대안으로 아래와 같은 방법이 있다.

- 오프셋을 이용한 중복 제거
  
  - 전송 데이터에 이름을 부여해 그것을 작은 메시지로 배송한다.
  
  - 각 메시지는 파일안의 시작 위치(오프셋)을 덧붙여, 메시지가 중복되어도 같은 파일의 같은 위치에 덮어쓰므로 중복 문제가 해결된다.
  
  - 벌크 형 메시지 전송에 적합하나 스트리밍 형에서는 쓰이지 않는다.

- 고유 ID에 의한 중복 제거
  
  - 스트리밍 형 메시지 배송에서 자주 사용되며 모든 메시지에 'UUID(universally Unique IDentifier)'와 같은 고유 ID를 저장한다.
  
  - 메시지가 늘어남에 따라 ID가 폭발적으로 증가하므로 최근에 받은 ID만을 기억하고 그보다 늦게 온 메시지의 중복은 허용하는 타협방안을 적용한다.

- 종단간(End to End)의 신뢰성
  
  - 성능과 신뢰성의 트레이트 오프 관계로..빅데이터의 메시지 배송에서는 종종 신뢰성보다 '효율'을 중시한다.
  
  - 중간 경로에 **'at least once'를 보장**하는 한편, '**중복 제거는 하지 않는 것**'이 **표준적인 구현**이다.

- 신뢰성이 높은 메시지 배송을 실현하려면 중간 경로를 모두 'at least once',로 통일한 후, 클라이언트 상에서 모든 메시지에 고유 ID를 할당하고 경로의 말단에서 중복 제거를 실행해야 한다.

- 고유 ID를 사용한 중복 제거의 방법
  
  - NoSQL 데이터베이스인 Cassandra와 Elasticsearch는 특성상 데이터를 쓸때 고유 ID를 지정해야 하므로 동일 ID의 데이터는 덮어쓰므로 중복 제거가 해결된다.
  
  - 보내온 데이터를 일단 그대로 객체 스토리지에 저장후, 읽어 들이는 단계에서 SQL을 사용하여 중복을 제거한다. 이는 대규모 데이터 처리이므로 메모리에서 실행이 불가능하므로 `Hive`같은 배치형 쿼리 엔진에서 실행한다.

#### 데이터 수집의 파이프라인

- 클라이언트 - 프론트엔드 - 메시지 브로커 - 소비자 - 분산 스토리지(- 중복 제거 - 데이터 구조화) 와 같은 일련의 프로세스로 장기적 데이터 분석을 위한 **'데이터 수집의 파이프라인'** 이 완성된다.

- 위 구조의 모든 요소를 구성하는게 아닌 요구 사항, 기존 시스템의 구조, 여러 환경 제약 등을 고려하여 필요와 목적에 맞게 파이프라인을 구성해야 한다.

- 중복을 고려한 시스템 설계
  
  - 빅데이터를 다루는 시스템은 매우 높은 성능을 요구하기 때문에 (필연적으로 발생하게 되는) 아주 작은 중복은 무시하는 경향이 있다.
  
  - 때문에 '중복이 있어도 문제가 되지 않는' 시스템을 설계하는게 중요하다.
  
  - 트랜잭션과 같이 신뢰성이 중시되는 경우 스트리밍 형 메시지 배송보다는 벌크 형을 선택하는게 좋다.

### 4-3. 시계열 데이터의 최적화

- 스트리밍 형의 메시지 배송에서 고려해야할 '메시지가 도착할 때까지의 시간 지연' 문제에 대해 알아보자

#### 프로세스 시간과 이벤트 시간

- 실시간으로 전송되는 데이터의 지연은 자주 발생한다.

- 클라이언트 상에서 메시지가 생성된 시간을 '**이벤트 시간(event time)**', 서버가 처리하는 시간을 '**프로세스 시간(process time)**'이라고 한다.
  
  - 데이터 분석 대상은 '이벤트 시간'으로 생성 이후 처리에 소모되는 프로세스 시간에 의해 문제가 발생한다.

#### 프로세스 시간에 의한 분할과 문제점

- 1월 1일에 발생한 이벤트를 수집한다고 했을때, 해당 일에 메시지를 수집한다고 해서 일괄적인 수집이 가능한게 아니다. 프로세스 시간에 의한 지연으로 이벤트 시간(1월 1일)에서 지난 시간에야 원하는 모든 이벤트를 수집할 수 있다.

- 분산 스토리지에 스트리밍 데이터를 저장하는 경우, 프로세스 시간을 기준으로 데이터를 저장한다.
  
  - 1월 1일의 이벤트를 수집하기 위해선 1달 이후인 2월 1일에 1월달에 저장된 모든 메시지를 검토하여 1월 1일 데이터를 추출한다.
  
  - 하지만 엄청난 양의 데이터를 특정 시간의 데이터를 검색하는건 시간과 자원을 낭비하는 처리이다.
  
  - 위와 같은 처리를 '**풀 스캔(full scan)**'이라고 한다. 이는 시스템 부하를 크게 높이는 요인 중 하나다.

#### 시계열 인덱스

- Cassandra와 같은 분산 데이터베이스에서는 데이터가 '**시계열 인덱스(time-serires index)**'로 대응하는 구조로 데이터 수집시 처음부터 이벤트 시간으로 인덱스 된 테이블을 만들 수 있다.

- 하지만 장기간에 대용량 데이터를 집계하는 경우 분산 데이터베이스는 효율적이지 않다.
  
  - 집계 효율을 높이기 위해 열 지향 스토리지를 지속적으로 만들어야 한다.

#### 조건절 푸쉬다운

- 매일 한번 씩 도착한 데이터를 배치 처리로 변환하여 지속적으로 열 지향 스토리지에 정렬된 형식으로 데이터를 저장할 수 있다.

- 이러한 열 지향 스토리지에는 '칼럼 단위의 통계 정보'를 이용한 최적화가 이뤄진다.
  
  - 동일 프로세스 시간에 처리된 메시지를 이벤트 시간 기준으로 정렬하여 저장 -> 원하는 데이터 추출 가능

- 필요한 최소한으 데이터를 읽도록 하는 최적화를 '**조건절 푸시 다운(predicate pushdown)**'이라고 한다.
  
  - 데이터가 정렬되어 저장된 열 지향 스토리지에서 조건절 푸시 다운을 통해 풀 스캔을 방지할 수 있다.

#### 이벤트 시간에 의한 분할

- 프로세스 시간으로 분할된 데이터베이스에 동일 이벤트 시간 데이터가 여러 파일로 분산된다.

- 이때 이벤트 시간 기준으로 다시 테이블을 분산 할 수 도 있다.
  
  - 테이블을 물리적으로 분리하는 테이블 파티셔닝 중 시간을 이용하여 분할된 테이블을 '**시계열 테이블(time-series table)**'이라고 한다.
  
  - 프로세스 시간으로 분리된 열 지향 스토리지에서 다시 이벤트 시간을 기준으로 시계열 테이블을 생성하면 쿼리 엔진은 특정 시간의 이벤트를 모든 열 지향 스토리지에서 찾는게 아닌 특정 시간의 시계열 테이블에서 데이터를 추출하면 된다.

- 이때 또 발생하는 문제가 1개의 열 지향 스토리지에는 넓은 범위의 이벤트 시간 데이터가 존재하고.. 그 만큼 자잘한 여러 시계열 테이블에 쓰기 작업을 실행한다.
  
  - 이로 인해 분산 스토리지에는 대량의 작은 파일이 생성되고 이는 쿼리엔진의 성능 악화로 이어진다.

#### 데이터 마트를 이벤트 시간으로 정렬하기

- 그래서 또 좋은 방법이 프로세스 시간으로 수집된 열 지향 스토리지에서 다시 이벤트 시간으로 분리된 여러 시계열 데이터로 저장하는게 아닌,

- 1개의 데이터 마트에 저장하면서 이벤트 시간에 의한 정렬을 함께 수행한다.

- 이러면 파일이 조각나 쿼리엔진에 부하를 발생하지 않고 항상 최적의 데이터 마트를 유지할 수 있다.
  
  - 아마 데이터 마트의 구조를 읽고쓰기에 특화되도록 구성해야 하지 않나 싶다.

### 4-4. 비구조화 데이터의 분산 스토리지

#### [기본 전략] NoSQL 데이터베이스에 의한 데이터 활용

- 빅데이터를 위한 분산 스토리지는 확장성과 어떤 데이터이든 저장 가능한 유연성이 요구된다. 

- 그 중 객체 스토리지는 파일을 교체하기 어렵다. 교체를 위해선 통째로 교체를 진행해야 한다.
  
  - 쓰기 작업을 위해 별도 RDB로 스냅샷하거나 다른 '**분산 데이터베이스(distributed database)**'에 저장하도록 한다.
  - 또한 저징 데이터르 집계할 수 있기 까지 열 지향 스토리지를 작성해야 하므로 그 시간이 오래 걸린다. 데이터 기록 직후 활용을 위해서는 실시간 집계와 검색에 적합한 데이터 저장소가 필요하다.

- "**특정 목적에 적합한 NoSQL 데이터베이스**"의 종류에 대해 알아보자.

#### 분산 KVS

- '**분산 KVS(distributed key-value store)**'는 모든 데이터를 키값 쌍으로 저장하도록 설계된 데이터 저장소를 말한다.

- 모든 데이터에 고유의 키를 지정하고 그것을 부하 분산을 위해 이용한다.
  
  - 고유 키가 정해지면 해당 값을 클러스터내의 어느 노드에 배치할 지 결정한다.

- 이러한 구조로 노드 간에 부하를 균등하게 분산하고 노드를 증감하는 것만으로 클러스터의 성능을 변경할 수 있다.
  
  - 마스터 - 슬레이브 구조에서는 마스터 노드가 shut down되면 데이터에 접근할 수 없지만,
  
  - P2P구조에서는 모든 노드가 대등한 관계이기 때문에 단일 실패점이 없다.

#### Amazon DynamoDB

- 클라우드 서비스에 NoSQL이 통합된 예로 AWS의 'Amazon DynamoDB'가 있다.

- DynamoDB는 항상 안정된 읽기 쓰기 성능을 제공하도록 디자인된 분산형 NoSQ 데이터베이스로 하나 또는 두 개의 키에 연결하는 형태로 임의의 스키마리스 데이터를 저장할 수 있다.

- 간단한 분산 KVS보다는 도큐먼트 스토어로 사용 가능하다.

- P2P 분산 아키텍쳐를 갖으며, 미리 설정한 초 단위의 요청 수에 따라 노드가 증감한다.
  
  - 급격한 요청에도 노드를 증가시켜 데이터의 읽기 및 쓰기에 지연을 방지한다.

- DynamoDB의 데이터를 분석하려면 동일 AWS 서비스인 Amazon EMR 및 Amazon Redshift등과 결합하여 Hive에 의한 배치 처리를 실행하거나 데이터 웨어하우스에 데이터를 전송하도록 한다. 또는 고유 기능인 'DynamoDB Streams'를 활용하여 데이터 변경 이벤트를 외부로 전송하여 실시간 스트림 처리가 가능하다.

#### 와이드 칼럼 스토어

- 분산 KVS에서 발전시켜 2개 이상의 임의의 키에 데이터를 저장할 수 있도록 한 것이 '**와이드 칼럼 스토어(wide-columns store)**'이다. 'Google Cloud Bigtable', 'Apache HBase', 'Apache Cassandra'등이 대표적이다.

- 와이드 컬럼 스토어에서는 내부적으로 행 키와 컬럼 명의 조합에 대해 값을 저장한다.
  
  - 테이블에 새로운 행 추가하는 것과 같이 칼럼도 얼마든지 추가할 수 있다.
  
  - 하나의 테이블에 가로와 세로의 2차원(또는 그 이상)에 데이터를 쓸 수 있다.

- 와이드 컬럼 스토어의 데이터 저장 방법
  
  |      | col1   | col2   | ... |
  | ---- | ------ | ------ | --- |
  | row1 | value1 | value2 |     |
  | row2 | value3 | value4 |     |
  | ...  |        |        |     |
  
    위와 같은 데이터를 아래와 같이 저장한다. 
  
  | 행 키  | 칼럼 명 | 값      |     |
  | ---- | ---- | ------ | --- |
  | row1 | col1 | value1 |     |
  |      | col2 | value2 |     |
  | row2 | col1 | value3 |     |
  |      | col2 | value4 |     |

#### Apache Cassandra

- 오픈 소스 와이드 칼럼 스토어로 'Apache Cassandra'가 있다.
- 내부적인 데이터 저장소로 와이드 칼럼 스토어를 이용하면서도 높은 수준의 'CQL'이라는 쿼리 언어가 구현되어 있다.(SQL과 비슷한 감각으로 사용가능)
- 테이블의 스키마를 결정할 필요가 있기 때문에 구조화 데이터만 취급할 수 있다.
  - RDB와 비슷해 보이지만 쿼리의 의미가 다르다. `INSERT INTO`는 '추가 또는 갱신(upsert)'로 동작해 동일한 키를 가진 레코드가 존재하면 덮어쓴다.
- P2P형 분산 아키텍처를 갖으며 지정 키에 의해 결정된 노드에 해당 키와 관련된 모든 값을 저장한다.
  - 사용자 ID를 키로 지정하면 해당 사용자에 대한 기록은 하나의 노드에만 모이고 해당 노드 안에서 쿼리가 실행된다.
  - 1억명의 활성 사용자가 있는 메시지 서비스에서 매일 수십억 레코드가 추가될 때, 사용자 ID를 키로 데이터를 분산하고 메시지의 타임스탬프로 레코드를 분리함으로써 사용자별 타임 라인을 구축할 수 있다.
    - CQL에서는 이러한 거대한 테이블을 '**복합 키(compound key)**'를 이용하여 실현한다.
- 와이드 컬럼 스토어도 분산된 모든 노드에서 데이터를 모아야 하므로 집계작업에 적합하지 않다. 이를 위해 Hive, Presto, Spark 등의 쿼리 엔진을 이용해 데이터를 추출해야 한다.

#### 도큐먼트 스토어

- '**도큐먼트 스토어(document store)**'는 JSON처럼 복잡하게 뒤얽힌 스키마리스 데이터를 그대로의 형태로 저장하고 쿼리를 실행한다.
  
  - 와이드 컬럼 스토어가 '성능 향상'을 목표로 하는 반면, 도큐먼트 스토어는 '데이터 처리의 유연성'을 목적으로 한다.

- 간단한 분산 KVS도 JSON텍스틀 저장할 수 있으나 그에 대한 복잡한 쿼리를 실행할 수 있다고 말할 수 없다.
  
  - 도큐먼트 스토어는 배열과 연상 배열과 같은 중첩 데이터 구조에 대해 인덱스를 만들거나 도큐먼트 일부를 치환하는 식의 쿼리를 쉽게 실행할 수 있다.

- 가장 큰 장점으로는 스키마를 정하지 않고 데이터 처리를 할 수 있다.
  
  - 외부 데이터를 저장하거나
  
  - 시스템의 데이터 및 로그 저장에 적합하다.

#### MongoDB

- 오픈 소스 분산형 도큐먼트 스토어로 'MongoDB'가 있다.

- 성능을 우선시 하고 신뢰성을 희생했지만, 간편하다.

- 여러 노드에 데이터를 분산할 수 있지만, 그 자체는 대량 데이터의 집계에 적합하지 않다.
  
  - 데이터 분석이 목적인 경우 쿼리 엔진으로 접속하여 데이터를 추출할 필요가 있다.

#### 검색 엔진

- '**검색 엔진(search engine)**'은 '**역 색인**'을 형성하여 텍스트 데이터 및 스키마리스 데이터를 집계하는 데 사용된다.
  
  - 역 색인으로 인해 데이터 기록 시스템의 시스템 부하 및 디스크 소비량은 커지지만 그 덕분에 키워드 검색이 훨씬 고속화된다.

- 장기적으로 데이터를 축적하기보다는 실시간 집계 시스템의 일부로 이용된다.

#### Elasticssearch

- 인기있는 오픈 소스 검색엔진으로 로그 수집 소프트웨어 'Logstash', 시각화 소프트웨어 'Kibana'와 함께 'ELK 스택' 또는 'Elastic 스택'으로 자주 이용된다.

- 임의의 JSON 데이터를 저장할 수 있기 때문에 도큐먼트 스토어와 비슷하지만, 아무것도 지정하지 않으면 모든 필드에 색인이 만들어진다.
  
  - 텍스트 데이터에서는 역 색인이 구축되어 간단한 도큐먼트 스토어와 비교하여 쓰기의 부하가 크고. 필요에 따라 명시적으로 스키마를 결정함으로 색인을 무효화하는 식의 튜닝이 필요하다.

- 자체 쿼리 언어에 의한 고급 집계 기능을 제공한다.
  
  - 표준 쿼리 언어를 사람의 힘으로 작성하는건 너무 복잡하므로 Kibana를 이용하거나 프로그램 내부에서 호출하는 것이 주요한 사용법이다.

#### Splunk

- '**Splunk(스플렁크)'** 는 상용 검색 엔진으로 텍스트 데이터를 집계하기 위한 도구이다.

- 웹 서버나 네트워크 기기로 출력되는 로그 파일이나 JSON 파일을 다루어 텍스트 처리한 비정형 데이터에 적합하다.
  
  - 키워드 입력시 해당 키워드를 포함하는 로그 정보를 찾는 등의 기능을 수행한다.

### 4-5. 정리

- 해당 장에서는 '**데이터를 모아서 분산 스토리지에 저장**'하기까지의 '**데이터 수집의 흐름**'을 설멍하었다.
  
  - 빅데이터의 효율적 집계를 위해 '**장기적인 데이터 분석**'을 가정한 '**스토리지**' 마련이 필수적이다.

- 너무 자주 데이터를 복사하면 데이터가 잘게 분열되어 '**집계 효율**'은 악화된다.
  
  - '**벌크 형의 데이터 전송**'이면 한번에 대량 데이터를 복사하기에 문제가 되지 않지만
  
  - '**스트리밍 형의 데이터 전송**'은 작은 메시지가 대량으로 들어오므로 그것을 정기적으로 모아서 기록하는 아이디어가 필요하다.

- '**다수의 클라이언트**'에서 실시간으로 '**데이터를 수집**'하는데 '**메시지 배송 방식**'이 사용된다.
  
  - '**메시지 브로커**'를 도입함으로 분산 스토리지에 쓰는 '**속도를 안정화**'할 수 있다.
  
  - 메시지를 여러 경로로 '**라우팅**'함으로 동일 데이터를 '**스트리밍 처리 및 배치 처리 모두에서 사용**'할 수 있다.

- 메시지 배송에서는 효율을 중시하여 '**트랜잭션 처리를 하지 않는 경우**'가 많아 잠재적으로 데이터가 중복되거나 누락될 가능성이 있다.
  
  - '**at least once**'를 통해 데이터의 누락을 피하지만 중복 제거의 경우 사용자의 책임이다.
  
  - 실제로 '**약간의 중복을 허용**'한 후에 '**신뢰성이 요구되는 부분에서 벌크 형의 데이터 전송**'하는게 일반적이다.

- 메시지 배송 방식이 아닌 '**NoSQL 데이터베이스와 같은 분산 스토리지**'에 직접 데이터를 쓰는 방법도 있다.
  
  - 읽기/쓰기엔 우수하지만 집계는 비효율적인 NoSQL에서는 '**집계**'를 위해서 '**쿼리엔진과 연결하여 애드 혹 분석**'을 하거나 '**정기적으로 데이터를 꺼내 장기적인 데이터 분석을 준비**'한다.

## 5장. 빅데이터의 파이프라인

- 빅데이터 파이프라인을 자동화하는 구조에 대해 알아보자.

### 5-1. [기초 지식] 워크플로 관리

- 정형적인 업무 프로세스(신청, 승인, 보고)와 같이 정해진 업무를 원활하게 진행하기 위한 구조

- 워크플로 관리 도구 : '정기적으로 태스크를 실행'하고 '비정상적인 상태를 감지하여 그것에 대한 해결을 돕는' 것
  
  - `Arifolw`, `Azkaban`, `Digdag`, `Luigi`, `Oozie` 등이 있다.

- **태스크(task)** : 데이터 파이프라인의 실행 과정의 정해진 처리

- 워크플로의 기본 기능과 빅데이터에서의 요구 기능
  
  - 파이프라인이 복잡해지거나 태스크의 수가 증가하면 실패 시 재실행이 어려워진다. 따라워크플로 관리 도구는 아래와 같은 기능을 제공한다.
    
    - 태스크를 정기적인 스케줄로 실행 후 결과 통지
    
    - 태스크간 의존 관계를 설정하고, 정해진 순서대로 빠짐없이 실행
    
    - 태스크의 실행결과를 보관하고, 오류 발생 시 재실행

- 선언 형과 스크립트 형

- 워크플로 관리 도구에는 '**선언형(declarative)**'와 '**스크립트 형(scripting)**'가 있다.
  
  - 선언형 : `XML`, `YAML`의 서식으로 워크플로를 기술한다.
  
  - 미리 제공된 기능만 이용할 수 있다.
  
  - 정해진 범위 내에서는 최소한의 기술로 태스크를 정의할 수 있다.
  
  - 누가 작성해도 동일한 워크플로가 되므로 유지 보수성이 높다.
  
  - 스크립트 형 : 스크립트 언어로 워크플로를 정의한다.
    
    - 일반 스크립트와 동일하게 변수와 제어 구문을 사용하여 태스크의 정의를 프로그래밍할 수 있어 유연하다.
    
    - 스크립트 언어에 의해 데이터 처리를 태스크 안에서 실행 가능
  
  - ETL 프로세스에는 스크립트 형의 도구, SQL의 실해에는 선언 형 도구를 사용하기도 한다.

#### 오류로부터의 복구 방법 먼저 생각하기

- 워크플로 관리에서는 태스크의 실행 순서를 정하는 것과 동시에 오류로부터 어떻게 회복할 것인가라는 계획을 정한다.

- 빅데이터를 처리하면서 네트워크, 스토리지, 성능 부족 등 여러 상황에서 오류가 발생할 수 있기 때문에.. 이에 대한 대처 방안을 고려하는게 중요하다.

- 복구와 플로우의 재실행
  
  - 워크플로 관리에서는 오류로부터 자동 회복할 수 있다는 점을 고려하지 않는다.
    
    - 대신 수작업에 의한 '**복구(recovery)**'를 전제한 태스크를 설계한다.
    
    - 실패 태스크를 모두 기록하여 그것을 나중에 재실행할 수 있도록 한다.
  
  - 워크플로 관리 도구에 의해 실행되는 일련의 태스크를 '**플로우(flow)**'라고 한다.
    
    - 각 플로우에는 실행 시 고정 파라미터가 부여된다. 이를 통해 동일 플로우에 동일 파라미터를 건네면 완전히 동일한 태스크가 실행되도록 한다.
  
  - 워크플로 관리 도구는 과거에 실행한 플로우와 그 파라미터를 자동으로 데이터베이스에 기록한다.
    
    - 플로우1에 태스크1-태스크2가 할당되어 있고, 플로우1 실행 중 태스크1 완료 후 태스크2에서 실패하면 해당 내용이 기록된다.
    
    - 이후 기록을 참고하여 플로우1 전체를 다시 실행하는게 아닌 태스크2만을 부분적으로 재실행하여 이전에 실패한 플로우1을 완료할 수 있다.

- 재시도
  
  - 여러 번 발생하는 오류에 대해서 자동화하여 수작업 없이 복구하기 위해 태스크 단위의 자동적인 '**재시도(retry)**'를 설정할 수 있다.
    
    - 대부분의 오류는 일시적인 오류가 많기 때문에,.. 5\~10분의 간격을 두고 재시도시 성공할 수 있다.
  
  - 재시도를 위한 그 반복 횟수가 중요하다. 너무 적으면 장애의 복구전 반복되어 의미가 사라지고, 너무 많으면 태스크가 실패하지 않는 것처럼 되어 중대한 문제로 이어진다.

- 백필
  
  - 실패한 플로우를 복구하는 다른 수단은 플로우 전체를 다시 실행할 수 있는 `**백필(backfill)** 기능
  
  - 파라미터에 포함된 일시를 순서대로 바꿔가면서 일정 기간의 플로우를 연속해서 실행하는 구조이다.
  
  - 대량의 태스크를 백필로 수행할때는 성능상의 주의를  필요로 한다. 매일 대규모로 처리하는 플로우를 한번에 처리하는 것이므로

#### 멱등한 조작으로 태스크를 기술하기

- 복구의 전제로서 기억해야 할 것은 재실행의 안정성
  
  - 태스크가 도중까지 실행하다 실패하여 그 경과가 남아있는 상태에서 태스크를 재실행한다면.. 혼재의 문제가 발생한다.
  
  - 태스크는 원칙적으로 '마지막까지 성공'하거나 '실패하면 아무것도 남지 않음' 둘 중 하나만 존재해야한다. '도중까지 성공'은 없다.

- 원자성 조작
  
  - `SQL`을 실행하는 태스크에서 `INSERT` 문을 2회 호출할때, 첫 번째는 성공했지만 두 번째에서 실패하여 이후 해당 태스크를 재실행하면 동일한 데이터가 다시 쓰이게 될 수 있다.
    
    - 트랜잭션에 대응되는 데이터베이스라면 트랜잭션을 통해 해당 문제를 방지할 수 있다.
    - 또는 시스템에 변경을 가하는 것을 기준으로 태스크를 분할하여 1개 태스크에는 1개의 변경 작업을 할당하는게 좋다. 이를 '**원자성 조작
      (atomic operation)**'이라고 한다.
  
  - 원자성 조작인 경우에도 문제가 발생한다. 
    
    - 원자성 조작 직후 문제가 발생하면, 원자성 조작 자체는 성공하고 있음에도 워크플로 관리 도구는 그것을 오류로 여기는 경우가 있다.
    
    - 이때 또한 중복으로 작업되는 문제가 발생할 수 있다.

- 멱등한 조작
  
  - 원자성 조작보다 더욱 확실한 것은 '동일한 태스크를 여러 번 실행해도 동일한 결과'가 되도록 하는 '**멱등한 조작(idempotent operation)**'이다. 
    
    - `SQL`의 예로 '테이블을 삭제한 후에 다시 만들기'가 있다.
      
      ```sql
      DROP TABLE IF EXISTS 'T1'; # 'T1' 테이블이 있다면 삭제한다.
      CREATE TABLE 'T1' (...); # 테이블 'T1'을 작성한다.
      INSERT INTO 'T1' ...; # 테이블 'T1'에 데이터를 쓴다.
      ```
  
  - 각 태스크를 어떻게 멱등하게 할지 생각하는 것은 이용자의 책임이다.
    
    - 일반적을 워크플로의 각 태스크는 '**추가(append)**'또는 '**치환(replace)**'중 하나를 실시한다.
  
  - 멱등한 태스크를 만들기 위해서는 태스크에 부여된 파라미터를 잘 이용해 고유의 이름을 생성하고, 여러 번 실행해도 항상 치환이 시행되도록 설계하면 된다.

- 멱등한 추가
  
  - 근데 현실은 항상 멱등한 태스크를 구현할 수 없다.
    
    - 과거의 모든 데이터를 치환하면 멱등하지만 -> 부하가 커진다.
    
    - `INSERT` 문 앞에 기존의 데이터를 `DELETE`하면 간접적으로 데이터를 치환하지만 -> 일반적으로 테이블로부터 일부 데이터를 삭제하는 건 비효율적이다.
  
  - '테이블 파티셔닝'을 통해 이를 해결할 수 있다.
    
    - 테이블을 1일 또는 1시간마다 파티션으로 분할하고, 파티션 단위로 치환하도록 한다.
    
    - 파티션의 모든 데이터를 삭제하는 데에는 `TRUNCATE`문이나 `INSERT OVERWRITE`문 등 효율 좋은 명령을 사용할 수 있다. - 태스크 단위로 멱등성을 유지하면서 보기에는 하나의 시계열 테이블에 데이터가 추가되는 듯한 워크플로 형성 가능

- 테이블 파티셔닝의 구현은 시스템에 따라 전혀 다르기 때문에.. 시스템 구성에 맞춰 플로우를 조립해야 한다.

- 원자성을 지닌 추가
  
  - 복잡한 플로우에서는 하나의 테이블에 몇 번이고 쓰기 작업을 실행할 수 있는데,
    
    - 이땐 추가를 반복하는게 아닌, 중간 테이블을 만들어 처리한 후 마지막에 목적 테이블에 추가함으로 트랜잭션과 같은 기능을 할 수 있다.
    
    ```sql
    DROP TABLE IF EXISTS 'T1';
    CREATE TABLE 'T1' (...);
    INSERT INTO 'T1' ...;
    INSERT INTO 'T1' ...; # 중간 테이블 T1 에 작성 
    
    INSERT INTO 'TARGET_TABLE'
    SELECT * FROM 'T1'; # 중간 테이블 T1의 데이터를 목적 테이블 TARGET_TABLE에 작성한다.
    ```

### 워크플로 전체를 멱등으로 하기

- 안정적인 데이터 파이프라인 운용을 위해서는 포함된 태스크나 플로우를 가능한 멱등하게 해야한다.
  
  - 데이터 수집(Data Ingestion)의 파이프라인에서는 테이블 파티셔닝으로 파티션 단위의 치환
  
  - 데이터 마트를 구축하는 플로우에서도 되도록 추가보다는 테이블마다 치환 또는 중간 테이블 활용

- 각 태스크를 멱등하게 하는 것이 이상적이지만 워크플로가 안정적으로 실행되는 한 필수는 아니다.
  
  - 추가가 문제되는 건 재시도 시의 중복의 가능성이다. 이를 주의하자.

- 재실행의 안정성을 높이기 위해 적어도 각 플로우가 전체로서 멱등하게 한다.
  
  - 중간 테이블을 초기화하는 태스크 이후 추가하는 태스크를 실행한다.

### 태스크 큐

- 워크플로 관리 도구에서 요구되는 다른 하나의 역할은 외부 시스템의 부하 컨트롤
  
  - 태스크의 크기나 동시 실행 수를 변화시킴으로 자원의 소비량을 조정하여 태스크들이 원활하게 실행되도록 한다.

- 예로 파일 서버로부터 분산 스토리지로의 파일 전송
  
  - 2MB의 압축이 안된 텍스트 파일이 1만개라면 20GB이다. 파일 1개당 압축 전송이 5초가 소요될때 1만 번 반복한다면 14시간이 걸린다.

- 이때 고려할 수 있는건 병렬화
  
  - '**잡 큐(job queue)**'나 '**태스크 큐(task queue)**'를 활용한다. 모든 태스크는 일단 큐에 저장한다.
  
  - 일정 수의 워커 프로세스가 순서대로 꺼내 실행하면서 병렬화가 실현된다.
    
    - 8코어 서버라면 각 코어마다 태스크가 할당되어 8개의 워커를 실행 할 수 있다.

- 병목 현상의 해소
  
  - 8코어 서버의 20개 워커도 감당 가능하나 각 태스크는 CPU 사용, 디스크 I/O, 네트워크 I/O를 소비하므로 어디선가 성능의 부하로 병목현상이 발생할 수 있다.
    
    - 서버의 내부적인 요인(메모리, CPU, 디스크, 네트워크의 자원 부족)
    
    - 외부적인 요인(파일 서버 측의 성능 한계) 가 발생한다.
  
  - 내부적인 요인은 해결이 가능하나.. 외부적인 요인은 해결이 불가하므로 병목현상을 방지하기 위해 적절한 워크를 설정하는게 좋겠다.

- 태스크수의 적정화
  
  - '하나의 파일 전송을 하나의 태스크'로 고려한 것이 문제다.
    
    - 작은 태스크를 다수 실행하면 오버헤드만 커진다.
    
    - 적정 태스크. 최적화를 고민해야한다.
  
  - 태스크에는 날짜와 시간이 파라미터로 건네진다. 이를 통해 각 태스크를 지정된 시간에 데이터를 모아 처리할 수 있다.
    
    - 1만개의 파일이 1년 걸려서 만들어진 것이라면, 태스크를 1일마다 나눔으로 1만개의 태스크를 365개의 태스크로 줄일 수 있다.
  
  - 작은 파일을 모아 하나의 파일로 처리하거나, 여러 파일을 한번에 업로드하여 태스크를 크게 하면 효율적으로 처리할 수 있다.
    
    - 좀 더 많은 워커로 동시에 실행함으로 전체로서 처리 효율을 최대화하는 조합을 찾아야 한다.

- 위와 같은 최적화 프로세스는 `Hadoop`과 같은 분산 시스템에서 변함없다.
  
  - 태스크가 너무 크면 나누고, 너무 작을 경우 하나로 모음으로 태스크가 **적절한 크기**가 될 수 있도록 조정한다.
  
  - 이후 한정된 자원 내에서 여러 태스크를 동시에 진행하도록 워커의 수를 늘려둔다.

- **최적화**를 통해 데이터 파이프라인 전체가 원활하게 실행되도록 제어하는 것, 워크플로 관리 도구의 역할 중 하나다.

### 5-2. 배치 형의 데이터 플로우

- DAG를 사용한 배치 형의 분산 데이터 처리의 사고방식 

#### MapReduce의 시대는 끝났다

- 분산 스토리지로의 데이터 전송이 완료되면, 분산 시스템의 프레임워크를 활용하여 데이터 처리가 가능하다.

- `MapReduce`를 사용한 데이터 처리에서는 `MapReduce`프로그램을 워크플로의 태스크로 등록함으로 다단계의 복잡한 데이터 처리를 했었다.
  
  - 기술의 발전으로 다단계의 데이터 처리를 그대로 분산 시스템 내부에서 실행할 수 있다. 이를 '**데이터 플로우(data flow)**'라고 지칭한다.

- 최근 경향으로 스트림 처리와 배치 처리가 하나로 통합되어 통일된 프레임워크로부터 양쪽이 모두 실행된다.
  
  - `Google Cloud Dataflow`, `Apache Spark`, `Apache Flink` 등이  있다.

- `MapReduce`의 구조
  
  - 한 때 빅데이터의 대표적인 기술
  
  - 데이터를 분할(split) -> 분산 노드로 병렬 작업 처리(map) -> 처리 결과를 집계(reduce)의 사이클로 구성된다.
  
  - 하나의 사이클이 끝나지 않으면 다음 사이클로 이동하지 않는다.
    
    ```python
    # a b
    # c b
    # a a 라는 데이터가 있을때, 각 알파벳의 개수를 세어보자
    
    # (a, b), (c, b), (a, a) 와 같이 행을 분할한다(split)
    
    # 모든 튜플은 병렬적으로 (알파벳, 1) 의 형태로 변환된다(map)
    # -> (a,1),(b,1)/ (c,1), (b,1)/ (a,1), (a,1) 로 변환 
    
    # 데이터를 모아 결과를 합산하여 집계한다(reduce)
    # (a,3), (b,2), (c,1) 
    
    # 결과값 저장
    ```

- 'Map과 Reduce를 반복한다'라는 사고 방식은 유효하지만 `MapReduce`를 사용하지는 않는다.

#### MapReduce를 대신할 새로운 프레임워크

- 여러 대체 프레임워크에는 공통적으로 '**DAG(directed acyclic graph 방향성 비순환 그래프)**'라는 데이터 구조가 사용된다.  

- DAG의 구조는
  
  - 노드와 노드가 단방향으로 연결되며(방향성)
  
  - 화살표를 아무리 따라가도 동일 노드로는 되돌아오지 않는다.(비순환)

- 데이터 플로우에서는 하나의 태스크를 하나의 노드, 의존 관계를 노드간 단방향 간선으로 표현할 수 있다.

- Spark에 있어서의 DAG
  
  - `Spark`에서도 DAG가 내부적으로 사용된다. 
    
    ```python
    lines = sc.textFile("sample.txt") # 데이터 불러오기
    
    words = lines.flatmap(lambda line : line.split()) # 행으로 분할(split)
    
    words.map(lambda word : (word,1))  # 변환 후 단어개수 카운팅(reduce)
         .reduceByKey(lambda a, b: a+ b)
         .saveAsTextFile("word_counts")# lazy evaluation에 의해 해당 부분에서 실제 계산 진
    ```
    
    위 와 같이 작업들은 의존관계를 갖고 진행된다.

- DAG에 의한 프로그래밍의 특징은 '**지연 평가(lazy evaluation)**'이다.
  
  - 위 스파크 예시와 같이 각 행은 DAG의 데이터 구조를 조립만 하지 어떤 값을 처리하지 않는다.
  
  - DAG를 구축하고 이후 명시적 혹은 암묵적으로 실행결과를 요구함에 따라 데이터 처리가 실행된다.

- `MapReduce`처럼 map과 reduce 마다 하나의 실행을 무조건 진행하는게 아닌, 일단 DAG로 전체를 조립 후 필요할때 실행에 옮김으로 내부 스케줄러가 분산 시스템에 효과적인 실행 계획을 세워주는 것이 데이터 플로우의 장점이다.

#### 데이터 플로우와 워크플로를 조합하기

- 데이터 플로우와 워크플로는 서로 보완 관계로 잘 나누어 사용해야 한다.
  
  - 태스크를 정기적으로 실행하거나, 실패 태스크를 기록하여 복구하는것은 데이터 플로우에서 불가능하나 워크플로는 가능하다.
  
  - 데이터 플로우의 프로그램을 워크플로의 일부로서 실행되는 하나의 태스크로 고려할 수 있다.

- 분산 시스템 안에서만 실행되는 데이터 처리라면, 하나의 데이터 플로우로 기술 가능하다.
  
  - 중간 테이블을 만들고, 그것을 다음의 쿼리로 읽어 들인다면(읽기, 쓰기 작업이 동일 노드에 존재) 다른 태스크로 분리할 필요가 없다.
  
  - 하지만 분산 시스템이 외부와 데이터를 주고 받는 경우, 어떤 오류가 발생할지 모르므로 복구를 고려하여 워크플로 안에 실행하는 게 바람직하다.

- 데이터를 읽어들이는 플로우
  
  - 데이터 플로우부터 읽어 들일 데이터는 성능적으로 안정된 분산 스토리지에 배치한다.
    
    - 데이터 플로우를 개발할때도 데이터를 여러번 읽어들여야 한다.
  
  - 외부의 데이터 소스에서 데이터를 읽어 들일 때는 벌크 형의 전송 도구로 태스크를 구현한다.
    
    - 데이터 소스에서의 읽기 속도는 한계가 있기 때문에, 데이터 플로우로 사용해도 그 속도가 빨라진다는 보장이 없다.
    
    - 따라서 읽기 속도보다는 오류의 발생에 확실하게 대처하여 복사를 끝내는게 우선이다.
    
    - 이를 위해 태스크 실행에는 워크플로 관리 도구를 사용하는게 적합하다.
  
  - 데이터의 복사만 완료되면 데이터 가공, 열 지향 스토리지로의 변환 등 부하가 큰 처리를 실행한다.
    
    - 위 작업을 하나의 태스크로 구현하면 정기적으로 데이터를 읽어 들이기 위한 워크플로가 완성된다.

- 데이터를 써서 내보내는 플로우
  
  - 데이터 집계 결과를 외부 시스템에 써서 내보내는 경우에는 읽어 들이는 과정과 반대의 관계가 있다.
  
  - 데이터 플로우 안에서 대량의 데이터를 외부에 전송하는 것은 피하는 편이 무난하다.
    
    - 쓰기 작업에 오랜 시간이 걸리면, 자원이 많이 허비되며 쓰기 작업을 실패하여 비용이 큰 작업을 재실행해야 할 수도 있다.
  
  - 데이터 플로우의 출력은 `CSV`파일과 같이 취급하기 쉬운 형식으로 변환하여 분산 스토리지에 써넣는다.
  
  - 외부 시스템에 데이터를 전송하는 건 워크플로의 역할이다.
    
    - 벌크 형의 전송 도구로 태스크를 구현하거나, 외부 시스템쪽에서 파일을 읽어 들이도록 지시한다.

#### 데이터 플로우와 SQL을 나누어 사용하기

- 데이터 입출력 파이프라인에 `SQL`에 의한 쿼리의 실행까지를 조합시킴으로써 배치형의 데이터 파이프라인이 완성된다.
  
  - 데이터 분석을 위해 데이터를 `SQL`로 쿼리를 실행시키는 일이 많은텐데 이것 또한 워크플로의 업무

- `SQL`을 MPP 데이터베이스에서 실행하는 경우와 분산 시스템상의 쿼리 엔진에서 실행하는 경우를 살펴보자.
  
  - 데이터 웨어하우스의 파이프라인(MPP 데이터베이스에서  `SQL`실행)
    
    - DB -> (벌크 전송) -> 분산 스토리지 -> (데이터 플로우) -> CSV -> (로드) -> 데이터 웨어하우스(<-> `SQL`에 의한 배치 처리) -> 시각화 도구
      
      - 워크플로가 포괄
    
    - 로드되는 데이터를 만드는 과정까지가 데이터 플로우의 역할, 그 이후의 태스크 실행이나 `SQL`에 의한 쿼리의 실행은 워크플로에 맡긴다.
  
  - 데이터 마트의 파이프라인(쿼리 엔진에서 `SQL` 실행)
    
    - DB -> (벌크 전송) -> 분산 스토리지 -> (데이터 플로우) -> 구조화 데이터 -> (쿼리엔진,`SQL` 사용) -> CSV -> (로드) -> 데이터 마트 -> 시각화 도구
      
      - 워크플로가 포괄
    
    - 구조화 데이터를 만드는 과정까지 데이터 플로우의 역할, 쿼리 엔진을 사용한 `SQL`실행이나 그 결과를 데이터 마트에 내보내는 것은 워크플로에서 실행한다.

- 대화식 플로우
  
  - 애드 혹 데이터 분석을 위해 데이터 플로우에 접속하여 스크립트 언어를 통한 raw데이터의 가공, 집계나
  
  - 쿼리 엔진에 접근하여 이미 구조화된 데이터에 접근할 수 있다.

#### 5-3. 스트리밍 형의 데이터 플로우

- 데이터의 실시간 처리를 높이려면, 배치 처리와는 전혀 다른 DAG를 사용한 스트림 처리 구조의 파이프라인이 필요하다.

#### 배치 처리와 스트림 처리로 경로 나누기

- 배치 처리 중심 데이터 파이프라인은 분석 가능한 데이터가 산출될때까지 '시간'이 소모된다.

- 실시간 처리에서는 배치 처리에서의 데이터 변환과정을 모두 생략한 다른 계통의 파이프라인을 만든다.
  
  - 이때 '실시간'이란 '이벤트 발생에서 몇 초 후에는 결과를 알 수 있는 것'을 가리킨다.

- 배치 처리와는 다르게 이벤트 발생으로 메시지 배송이 이뤄질때, 분산 스토리지을 거치지 않고 처리를 계속하는 것이 **스트림 처리(streaming processing)** 이라고 한다.
  
  - 배치 처리에서는 도달한 데이터를 우선 분산 스토리지에 저장하고, 그것을 정기적으로 추출하여 데이터 처리를 한다. -> 과거 데이터에 재작업 가능
  
  - 스트림 처리에서는 데이터가 도달함과 거의 동시에 처리가 시작된다. -> 과거 데이터에 재작업 불가

#### 배치처리와 스트림 처리 통합하기

- 데이터를 작게 나눠 DAG에 흘려넣는 배치 처리와는 다르게, 스트림 처리에서는 데이터가 끊임없이 생성되며 즉시 DAG에 흘러들어간다.
  
  - 배치 처리와 같이 실행 시 데이터양이 정해진 '**유한 데이터(bounded data)**'
  
  - 스트림 처리와 같이 제한이 없이 데이터가 보내지는 '**무한 데이터(unbounded data)**'

- `Spark Streaming`의 DAG
  
  - 배치 처리를 위해 설계된 분산 시스템인 `Spark`에서는 `Spark Streaming`이라는 기능이 통합되었다.
  
  - 배치 처리를 위한 `Spark`의 스크립트를 조금만 수정하면 `Spark Streaming`에서도 사용 가능하다.

#### 스트림 처리의 결과를 배치 처리로 치환하기

- 스트림 처리의 잠재적인 두 가지 문제
  
  - 틀린 결과를 어떻게 수정할 것인가? - '시간을 되돌린다'라는 개념은 없음 도달한 데이터는 바로 처리되어 사라짐
  
  - 늦게 전송된 데이터는 어떻게 취급할 것인가? - 메세지 배송의 지연 발생

- 전통적인 대처 방법은 스트림 처리와 별개로 배치 처리를 실행하여 후자의 결과를 옳다고 하는것.

- 람다 아키텍쳐
  
  - 전통적인 방법에서 발전시킨 방법, '**람다 아키텍쳐(lambda architecture)**'. 3개의 레이어로 구분된다.
  
  - 모든 데이터를 처리하는 '**배치 레이어(batch layer)**'. 과거의 데이터를 장기적인 스토리지에 축적한다.
  
  - 배치 처리 결과를 '**서빙 레이어(serving layer)**'를 통해서 접근한다. 
    
    - 응답이 빠른 데이터베이스를 설치하여 집계 결과를 바로 추출한다.
    
    - 서빙 레이어에서 얻어진 결과를 '**배치 뷰(batch view)**'라고 한다.
    
    - 정기적으로 업데이트되지만, 실시간 정보는 얻을 수 없다.
  
  - 스트림 처리를 위한 '**스피드 레이어(speed layer)**'
    
    - 스피드 레이어에서 얻은 결과를 '**실시간 뷰(realtime view)**'라고 한다.
    
    - 실시간 뷰는 배치 뷰가 업데이트될 동안까지만 이용되고, 오래된 데이터는 순서대로 삭제된다.
  
  - 배치 부와 실시간 뷰 모두를 조합시키는 형태로 쿼리를 실행한다.
    
    - 최근 24시간 집계결과는 실시간 뷰를 참고하고, 그 이전의 데이터는 배치 뷰를 사용한다.
    
    - 배치 처리와 스트림 처리의 결점을 보완할 수 있다.
  
  - 람다 아키텍쳐의 장점은 실시간 뷰의 결과가 나중에 배치 뷰로 치환된다는 것
    
    - 스트림 처리 결과는 일시적으로 사용되므로 혹여나 잘못되어도 배치 처리 결과에 의해 옳은 결과를 얻을 수 있다.
    
    - 배치 처리만 안정되게 동작하고 있다면 '**스트림 처리를 다시 실행할 필요가 없다**'가 람다 아키텍쳐의 개념이다.

- 카파 아키텍쳐
  
  - 람다 아키텍쳐는 스피드 레이어와 배치 레이어가 동일한 처리를 구현하므로 개발 효율이 좋지 않다.
  
  - 이에 따라 람다 아키텍쳐를 단순화한 '**카파 아키텍쳐(kappa architecture)**'가 선택될 수 있다.
  
  - 카파 아키텍쳐에서는 람다 아키텍쳐의 배치 레이어나 서빙 레이어를 완전히 제거하고, 스피드 레이어만 남긴다.
    
    - 대신 메시지 브로커의 데이터 보관 기간을 충분히 하여, 문제 발생시 메시지 배송 시간을 과거로 다시 설정한다.
    
    - 재설정된 시간에 의해 과거 데이터가 실시간 처리로 흘러 들어 실질적인 재실행이 이뤄진다.
  
  - 스트림 처리의 데이터 플로우에 대량의 과거 데이터를 흘려보내면, 평상시에 비해 몇 배 이상의 계산 자원을 일시적으로 소비한다는 문제점이 있다.
  
  - 필요에 따라서 '스트림 처리를 다시 하는 것이 간단하다'는 것이 카파 아키텍쳐의 개념이다.

#### 아웃 오브 오더의 데이터 처리

- 스트림 처리에서는 늦게 도달하는 메시지, 프로세스 시간과 이벤트 시간의 차이로 문제가 발생한다. 
  
  - 기술적으로 '**아웃 오브 오더(out of order)**'의 데이터 문제라고 불린다.

- 원래 데이터의 모습은 '이벤트 시간'으로 얻을 수 있다.
  
  - 스트림 처리란 기본적으로 프로세스 시간에 의한 실시간 데이터 처리
  
  - 데이터가 도달한 순간 집계를 시작하므로, 별다른 조작을 하지 않는 한 해당 출력은 프로세스 시간과 연관된다.
  
  - 하지만 어떤 이유로 스트림 처리가 일시적으로 멈춘 후 재기동하면 쌓여 있던 데이터 처리가 재개된다.
    
    - 이때 양상을 시각화하면 흘러드는 데이터양에 변화가 있는 것 처럼 보인다.
    
    - 지연이 발생할때마다 스트림 처리의 결과가 요동치게 되고, 결과의 신뢰성은 떨어진다.
  
  - 프로세스 시간으로 집계하는 한 원래 데이터의 모습을 알 수 없으니, 데이터가 처음으로 생성된 시간 '이벤트 시간'으로 집계해야 올바른 결과를 얻는다.

- 이벤트 시간 윈도윙
  
  - 스트림 처리에서는 시간을 일정 간격으로 나누어 '**윈도우(window)**'를 만들고 그 안에서 데이터를 집계한다.
    
    - 1시간의 이벤트 수 추이를 그래프로 만든다면, 1분 간격인 60개의 윈도우로 나누어 각각의 윈도우로 이벤트 수를 센다.
  
  - 이벤트 시간에 의해 윈도우를 나누는 것을 '**이벤트 시간 윈도윙(event-time windowing)**'이라고 한다.
    
    - 메시지가 배송된 데이터는 무작위 순으로 나열된 '아웃 오브 오더' 상태로 적절히 sort하여 집계결과를 업데이트 한다.
    
    - 실시간 데이터는 계속 생성되어 계속 메시지 배송이 반복되므로..데이터 도달할때마다 재집계를 해줘야 한다.

### 5-4. 정리

- '**빅데티어의 데이터 파이프라인**'을 구축할 때의 기술로 '**워크플로**'와 '**데이터플로우**'의 개념

- '**워크플로 관리 도구**'는 여러 시스템에 명령하기 위한 '**사령탑**'의 역할, '**각종 태스크의 스케줄 실행**' 및 '**오류로부터의 복구**'를 돕는다.

- 빅데이터의 '**집계**'에는 '**장애**'가 발생할 수 있다. 
  
  - 만일을 위해 가능한 한 '**멱등한 태스크**'를 구현하여 '**복구 가능한 워크플로**'를 작성하자.

- 워크플로 관리 도구는 '**외부 시스템에 영향을 미치는 부하를 조정하는 역할**'도 담당한다.
  
  - 태스크의 크기, 동시 실행 수를 제어하여 '**안정적인 태스크 실행**'과 '**자원의 유효한 활용**'을 양립하도록 한다. -> 최적화가 필요하다!

- 분산 스토리지에 데이터를 넣은 후에는 '**데이터 플로우**'가 등장한다.
  
  - '**DAG**'라는 형태로 플로우를 기술함으로 '**분산 시스템 내부에서 높은 효율로 실행**'할 수 있다.
  
  - '**배치 형의 데이터 플로우를 스크립트화**'해두면 '**데이터의 구조화**'나 '**데이터 마트의 구축**'이라는 프로세스를 '**단순 태스크로 워크플로에서 호출**'할 수 있다.

- '**실시간 데이터 처리**'를 위해서는 '**스트리밍 형의 데이터 플로우**'를 실행한다.
  
  - 스트림 처리는 과거를 되돌리기 어려우므로, 필연적으로 '**배치 처리**'와 조합하여 '**2 계통의 데이터 처리**'를 하게 된다.
  
  - 이는 일반적으로 '**람다 아키텍쳐**'로 알려져 있으며, 시스템을 복잡하게 하므로 '**스트림 처리의 도입**'은 신중해야 한다.

- '**데이터 파이프라인의 안정적인 운용**'을 위해서는 '**워크플로 관리가 필수적**'이므로, '**워크플로 관리 도구**'를 제대로 사용하여 '**안정성**'을 높이는 것이 중요하다.

## 6장. 빅데이터 분석 기반의 구축

- 

---

- 레퍼런스

> 빅데이터를 지탱하는 기술
