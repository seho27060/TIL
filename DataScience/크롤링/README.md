[TOC]

# 크롤링 흐름

### Phase 1

1. 상권데이터 300,000개 중  "상호 대분류" == "음식" 으로 필터링, 100,000개 데이터 추출

2. 1.의 데이터에서 "상호명"을 query로 네이버 블로그 or 네이버 지도 리뷰 검색 크롤링
   
   - 분산 크롤링으로 실행.
   
   - 논의점1: 병렬 크롤링 적용 여부

3. 2.의 검색 결과를 토대로 1차 시드 URL 수집
   
   - 논의점1 : scrapy, selenium, bs4 중 어떤 패키지 사용
   
   - 논의점2 : robots.txt 준수 여부. ip차단시 해결방안 고려.

### Phase 2

4. 1개 음식점 별 리뷰 데이터를 50개씩 수집한다는 가정하에, 100,000 \* 50 개의 시드 URL 형성

5. 시드 URL들에 대해 리뷰 관련 url 탐색
   
   - 논의점1: 개별 URL 웹 페이지내의 URL 재탐색 하지않음/ 사용에 따른 수정 필요

6. 리뷰 관련 url 크롤링을 통한 해당 웹 페이지의 본문을 텍스트 데이터로 추출
   
   - 논의점1 : 추출 데이터에 대한 신뢰성, 해당 데이터가 검색한 "상호명"과 연관이 있는가?

### Phase 3

7. \~Phase2 의 결과로 100,000개 "상호명" 별 음식점에 대한 리뷰 데이터 적재.

8. 각 리뷰 데이터에서 형태소분석을 통한 키워드 추출 - 테이블에 저장

9. 각 리뷰 데이터에서 감성분석으로 리뷰에 따른 인기 점수 산정 - 테이블에 저장
- 5,000,000만개 데이터 순회하면서 감성분석 + 키워드 추출 진행





#### 행에 대한 추가 지표 생성

- 리뷰수.

- 감성 점수

- 키워드
